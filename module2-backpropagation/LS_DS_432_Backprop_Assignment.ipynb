{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_432_Backprop_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cocoisland/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/module2-backpropagation/LS_DS_432_Backprop_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NGGrt9EYlCqY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Backpropagation Practice\n",
        "\n",
        "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 0  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 1 |\n",
        "| 0  | 1  | 0  | 1 |\n",
        "| 1  | 0  | 0  | 1 |\n",
        "| 1  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 0  | 0 |\n",
        "\n",
        "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
      ]
    },
    {
      "metadata": {
        "id": "nEREYT-3wI1f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(object):\n",
        "  def __init__(self):\n",
        "    self.inputs = 3\n",
        "    self.hiddenNodes = 4\n",
        "    self.outputNodes = 1\n",
        "\n",
        "    # Initlize Weights\n",
        "    self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes) # (3x4)\n",
        "    self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes) # (4x1)\n",
        "\n",
        "  def feed_forward(self, X):\n",
        "    # Weighted sum between inputs and hidden layer\n",
        "    self.hidden_sum = np.dot(X, self.L1_weights)\n",
        "    # Activations of weighted sum\n",
        "    self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "    # Weighted sum between hidden and output\n",
        "    self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
        "    # final activation of output\n",
        "    self.activated_output = self.sigmoid(self.output_sum)\n",
        "    return self.activated_output\n",
        "    \n",
        "  def sigmoid(self, s):\n",
        "    return 1/(1+np.exp(-s))\n",
        "  \n",
        "  def sigmoidPrime(self, s):\n",
        "    return s * (1 - s)\n",
        "  \n",
        "  def backward(self, X, y, o):\n",
        "    # backward propgate through the network\n",
        "    self.o_error = y - o # error in output\n",
        "    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
        "\n",
        "    self.z2_error = self.o_delta.dot(self.L2_weights.T) # z2 error: how much our hidden layer weights contributed to output error\n",
        "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden) # applying derivative of sigmoid to z2 error\n",
        "\n",
        "    self.L1_weights += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
        "    self.L2_weights += self.activated_hidden.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
        "    \n",
        "  def train (self, X, y):\n",
        "    o = self.feed_forward(X)\n",
        "    self.backward(X, y, o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vNytLScaDCSF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "# X = (hours studying, hours sleeping), y = score on test\n",
        "X = np.array(([0,0,1],[0,1,1],[1,0,1],[0,1,0],[1,0,0],[1,1,1],[0,0,0]), dtype=float)\n",
        "y = np.array(([0],[1],[1],[1],[1],[0],[0]), dtype=float)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KuL7nLtyCZNV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12446
        },
        "outputId": "899a2c1f-c024-4d21-db6e-07f441a6b53a"
      },
      "cell_type": "code",
      "source": [
        "NN = Neural_Network()\n",
        "for i in range(1000): # trains the NN 1,000 times\n",
        "  if i+1 in [1,2,3,4,5] or (i+1) % 50 == 0:\n",
        "    print('+---------- EPOCH', i+1, '-----------+')\n",
        "    print(\"Input: \\n\", X) \n",
        "    print(\"Actual Output: \\n\", y)  \n",
        "    print(\"Predicted Output: \\n\" + str(NN.feed_forward(X))) \n",
        "    print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.feed_forward(X))))) # mean sum squared loss\n",
        "    print(\"\\n\")\n",
        "  NN.train(X, y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 1 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.6085491 ]\n",
            " [0.67987193]\n",
            " [0.59190328]\n",
            " [0.58737534]\n",
            " [0.43434821]\n",
            " [0.67245612]\n",
            " [0.41670933]]\n",
            "Loss: \n",
            "0.2507745536252601\n",
            "\n",
            "\n",
            "+---------- EPOCH 2 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.60571618]\n",
            " [0.68147477]\n",
            " [0.59781554]\n",
            " [0.59108426]\n",
            " [0.44510504]\n",
            " [0.6772189 ]\n",
            " [0.41631019]]\n",
            "Loss: \n",
            "0.248166125341833\n",
            "\n",
            "\n",
            "+---------- EPOCH 3 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.60177983]\n",
            " [0.68201444]\n",
            " [0.60214001]\n",
            " [0.59345466]\n",
            " [0.45422185]\n",
            " [0.68065578]\n",
            " [0.41469106]]\n",
            "Loss: \n",
            "0.2457086042973095\n",
            "\n",
            "\n",
            "+---------- EPOCH 4 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.59725202]\n",
            " [0.68204056]\n",
            " [0.60551682]\n",
            " [0.5951023 ]\n",
            " [0.46222963]\n",
            " [0.68339033]\n",
            " [0.41231925]]\n",
            "Loss: \n",
            "0.24337054256476656\n",
            "\n",
            "\n",
            "+---------- EPOCH 5 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.59242099]\n",
            " [0.68186507]\n",
            " [0.60832696]\n",
            " [0.59638352]\n",
            " [0.46946198]\n",
            " [0.68578114]\n",
            " [0.4094688 ]]\n",
            "Loss: \n",
            "0.2411310794187638\n",
            "\n",
            "\n",
            "+---------- EPOCH 50 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.2929504 ]\n",
            " [0.74956594]\n",
            " [0.72852226]\n",
            " [0.72098771]\n",
            " [0.70476307]\n",
            " [0.82569688]\n",
            " [0.22562504]]\n",
            "Loss: \n",
            "0.1599902870698427\n",
            "\n",
            "\n",
            "+---------- EPOCH 100 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.14036303]\n",
            " [0.798659  ]\n",
            " [0.79619668]\n",
            " [0.79045771]\n",
            " [0.78968642]\n",
            " [0.86343467]\n",
            " [0.13529971]]\n",
            "Loss: \n",
            "0.13624871278675393\n",
            "\n",
            "\n",
            "+---------- EPOCH 150 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.10060864]\n",
            " [0.80205117]\n",
            " [0.80264794]\n",
            " [0.79426731]\n",
            " [0.79692753]\n",
            " [0.85062128]\n",
            " [0.10015419]]\n",
            "Loss: \n",
            "0.12934363772916874\n",
            "\n",
            "\n",
            "+---------- EPOCH 200 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.08360886]\n",
            " [0.79708068]\n",
            " [0.7994544 ]\n",
            " [0.79050658]\n",
            " [0.79701022]\n",
            " [0.83428384]\n",
            " [0.08274976]]\n",
            "Loss: \n",
            "0.12519351729897774\n",
            "\n",
            "\n",
            "+---------- EPOCH 250 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.07326386]\n",
            " [0.78942227]\n",
            " [0.79481903]\n",
            " [0.78563543]\n",
            " [0.79875539]\n",
            " [0.81849742]\n",
            " [0.07196999]]\n",
            "Loss: \n",
            "0.12191129619572597\n",
            "\n",
            "\n",
            "+---------- EPOCH 300 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.0656122 ]\n",
            " [0.77705243]\n",
            " [0.79021845]\n",
            " [0.77777167]\n",
            " [0.80482684]\n",
            " [0.79990191]\n",
            " [0.06369454]]\n",
            "Loss: \n",
            "0.11848527548224982\n",
            "\n",
            "\n",
            "+---------- EPOCH 350 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.05951567]\n",
            " [0.75483413]\n",
            " [0.79130333]\n",
            " [0.7622785 ]\n",
            " [0.82066599]\n",
            " [0.77361614]\n",
            " [0.0558312 ]]\n",
            "Loss: \n",
            "0.11392485219091265\n",
            "\n",
            "\n",
            "+---------- EPOCH 400 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.05504447]\n",
            " [0.72659734]\n",
            " [0.80563099]\n",
            " [0.7420325 ]\n",
            " [0.84622939]\n",
            " [0.74644664]\n",
            " [0.0489326 ]]\n",
            "Loss: \n",
            "0.1093325470588644\n",
            "\n",
            "\n",
            "+---------- EPOCH 450 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.05160153]\n",
            " [0.70852083]\n",
            " [0.81453441]\n",
            " [0.73301414]\n",
            " [0.86652609]\n",
            " [0.72835865]\n",
            " [0.0449675 ]]\n",
            "Loss: \n",
            "0.1062350641928191\n",
            "\n",
            "\n",
            "+---------- EPOCH 500 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.04879251]\n",
            " [0.69833358]\n",
            " [0.814513  ]\n",
            " [0.73387099]\n",
            " [0.87823652]\n",
            " [0.71464453]\n",
            " [0.0422256 ]]\n",
            "Loss: \n",
            "0.10370565291085133\n",
            "\n",
            "\n",
            "+---------- EPOCH 550 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.04676793]\n",
            " [0.69291808]\n",
            " [0.80667536]\n",
            " [0.7431952 ]\n",
            " [0.88001172]\n",
            " [0.70142927]\n",
            " [0.0394352 ]]\n",
            "Loss: \n",
            "0.10110928675825735\n",
            "\n",
            "\n",
            "+---------- EPOCH 600 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.04628586]\n",
            " [0.6967563 ]\n",
            " [0.7875745 ]\n",
            " [0.76380106]\n",
            " [0.86467527]\n",
            " [0.68428394]\n",
            " [0.03575276]]\n",
            "Loss: \n",
            "0.09754988640849356\n",
            "\n",
            "\n",
            "+---------- EPOCH 650 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.04918066]\n",
            " [0.72256424]\n",
            " [0.76157337]\n",
            " [0.78758784]\n",
            " [0.82555479]\n",
            " [0.65988454]\n",
            " [0.03098578]]\n",
            "Loss: \n",
            "0.09259919629441955\n",
            "\n",
            "\n",
            "+---------- EPOCH 700 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.0523566 ]\n",
            " [0.74709748]\n",
            " [0.75569829]\n",
            " [0.79111654]\n",
            " [0.79728572]\n",
            " [0.63441253]\n",
            " [0.02722211]]\n",
            "Loss: \n",
            "0.0877614149956932\n",
            "\n",
            "\n",
            "+---------- EPOCH 750 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.05119722]\n",
            " [0.75801268]\n",
            " [0.76214989]\n",
            " [0.78702829]\n",
            " [0.78898553]\n",
            " [0.61124885]\n",
            " [0.02451373]]\n",
            "Loss: \n",
            "0.08312311826561582\n",
            "\n",
            "\n",
            "+---------- EPOCH 800 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.04781202]\n",
            " [0.76523518]\n",
            " [0.76819356]\n",
            " [0.78567624]\n",
            " [0.78682241]\n",
            " [0.58943997]\n",
            " [0.02230775]]\n",
            "Loss: \n",
            "0.07863588654179895\n",
            "\n",
            "\n",
            "+---------- EPOCH 850 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.04418313]\n",
            " [0.77109904]\n",
            " [0.77331774]\n",
            " [0.78650498]\n",
            " [0.78724262]\n",
            " [0.56864439]\n",
            " [0.02046134]]\n",
            "Loss: \n",
            "0.07433622515899495\n",
            "\n",
            "\n",
            "+---------- EPOCH 900 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.0409006 ]\n",
            " [0.77634051]\n",
            " [0.77804641]\n",
            " [0.78855514]\n",
            " [0.78904805]\n",
            " [0.5488273 ]\n",
            " [0.01889405]]\n",
            "Loss: \n",
            "0.0702482660105808\n",
            "\n",
            "\n",
            "+---------- EPOCH 950 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.03802592]\n",
            " [0.78128867]\n",
            " [0.78262851]\n",
            " [0.79131969]\n",
            " [0.79165742]\n",
            " [0.53000766]\n",
            " [0.01754758]]\n",
            "Loss: \n",
            "0.06638587414129424\n",
            "\n",
            "\n",
            "+---------- EPOCH 1000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.03550148]\n",
            " [0.78607767]\n",
            " [0.78715019]\n",
            " [0.79452007]\n",
            " [0.79475567]\n",
            " [0.51219048]\n",
            " [0.01637882]]\n",
            "Loss: \n",
            "0.06275467814239669\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8b-r70o8p2Dm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Try building/training a more complex MLP on a bigger dataset.\n",
        "\n",
        "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
        "\n",
        "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
      ]
    },
    {
      "metadata": {
        "id": "5MOPtYdk1HgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc5df141-2edb-487a-e93b-2bc3282dadef"
      },
      "cell_type": "code",
      "source": [
        "# Tensorflow and Keras API to import and download the MNIST dataset \n",
        "# MNIST - 60,000 training images and 10,000 testing, \n",
        "# X train - RBG greyscale (0-255)\n",
        "# y train - labe 0-9\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "U1NlWvYPLZcw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8762297f-a354-406d-d8ba-728502945d76"
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "g_zTw3TiLTPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "dab232b2-eb47-43f8-b3a5-e438b6757245"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline # Only use this if using iPython\n",
        "image_index = 7777 # You may select anything up to 60,000\n",
        "print(y_train[image_index]) # The label is 8\n",
        "plt.imshow(x_train[image_index], cmap='Greys')\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5b8a34a4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "F6M3xNanMsAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "576b87b2-ba5f-4146-81c6-0e1b8d339ac7"
      },
      "cell_type": "code",
      "source": [
        "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "# Making sure that the values are float so that we can get decimal points after division\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('Number of images in x_train', x_train.shape[0])\n",
        "print('Number of images in x_test', x_test.shape[0])\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "Number of images in x_train 60000\n",
            "Number of images in x_test 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lyxm1xOHM752",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "9912231d-2a1b-4001-85ed-836cfa141c7a"
      },
      "cell_type": "code",
      "source": [
        "# Building convolutional neural network\n",
        "# Importing the required Keras modules containing model and layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "# Creating a Sequential Model and adding the layers\n",
        "model = Sequential()\n",
        "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model.add(Dense(128, activation=tf.nn.relu))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10,activation=tf.nn.softmax))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3qLxvt_rNw9y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "a11accc5-efaf-471b-acdf-33210b73ecd2"
      },
      "cell_type": "code",
      "source": [
        "# Compile and fit the model\n",
        "# adam optimizer is usually out-performs the other optimizers. \n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "model.fit(x=x_train,y=y_train, epochs=10)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 59s 982us/step - loss: 0.1989 - acc: 0.9407\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 60s 997us/step - loss: 0.0799 - acc: 0.9754\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 57s 955us/step - loss: 0.0540 - acc: 0.9826\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 57s 953us/step - loss: 0.0414 - acc: 0.9867\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 57s 953us/step - loss: 0.0342 - acc: 0.9887\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 57s 954us/step - loss: 0.0280 - acc: 0.9904\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 59s 977us/step - loss: 0.0231 - acc: 0.9923\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 57s 954us/step - loss: 0.0192 - acc: 0.9934\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 57s 952us/step - loss: 0.0201 - acc: 0.9933\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 57s 955us/step - loss: 0.0180 - acc: 0.9939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5b8a3306a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "Cj_kvNsXPNrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aea63f5c-d8a5-4ef1-f5c9-3058e818bb6e"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 270us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06118579938681469, 0.9862]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "gWDFx-32PUjh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "dcd9d2ef-c6ef-4732-8494-e42bd6598b99"
      },
      "cell_type": "code",
      "source": [
        "image_index = 7777\n",
        "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
        "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
        "print(pred.argmax())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSAC1Lh7VAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_A5CD0aEJgg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa5c1106-9414-4330-c014-976a43340fcb"
      },
      "cell_type": "code",
      "source": [
        "#http://rasbt.github.io/mlxtend/user_guide/data/mnist_data/\n",
        "\n",
        "from mlxtend.data import mnist_data\n",
        "X, y = mnist_data()\n",
        "\n",
        "print('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions: 5000 x 784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CKoOZXrkKKEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "87ec1b46-8974-4e0e-86b8-e496dbb1554d"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_digit(X, y, idx):\n",
        "    img = X[idx].reshape(28,28)\n",
        "    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n",
        "    plt.title('true label: %d' % y[idx])\n",
        "    plt.show()\n",
        "plot_digit(X, y, 4000)     "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAEHCAYAAACHl1tOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "FwlRJSfBlCvy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch Goals: \n",
        "\n",
        "- Implement Cross Validation model evaluation on your MNIST implementation \n",
        "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
        " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
        "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
      ]
    }
  ]
}